{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e732e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "#　ライブラリのインポート\n",
    "#------------------------------------------\n",
    "# ライブラリのインポート\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# langchain のインポート\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85828e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyA7ff88iym-pPHbjSKf-x6ddwFYzULZ9ec\")\n",
    "gemini=genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c6e3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "#　LLMの出力を表示する関数。Markdownテキストを受け取り、HTMLに変換して表示。\n",
    "#------------------------------------------\n",
    "def display_markdown(md_text):\n",
    "    html_output = markdown.markdown(md_text)\n",
    "    display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b2414d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>LLM（大規模言語モデル）とは？</h2>\n",
       "<p>LLM（Large Language Model、大規模言語モデル）とは、<strong>大量のテキストデータを学習することで、人間が書くような自然な文章を生成したり、様々な言語タスクを実行できるAIモデル</strong>のことです。</p>\n",
       "<p><strong>キーポイント:</strong></p>\n",
       "<ul>\n",
       "<li><strong>大規模:</strong> 非常に多くのデータ（数十億から数兆の単語）を学習しています。</li>\n",
       "<li><strong>言語:</strong> テキストデータを理解し、生成することに特化しています。</li>\n",
       "<li><strong>モデル:</strong> ニューラルネットワークという機械学習の技術を利用しています。</li>\n",
       "</ul>\n",
       "<p><strong>LLMができること（例）：</strong></p>\n",
       "<ul>\n",
       "<li><strong>テキスト生成:</strong> ストーリー、詩、ニュース記事、ブログ記事などを生成できます。</li>\n",
       "<li><strong>テキスト要約:</strong> 長い文章を要約し、重要なポイントを抽出できます。</li>\n",
       "<li><strong>質問応答:</strong> 質問に対して、テキストデータに基づいた回答を生成できます。</li>\n",
       "<li><strong>翻訳:</strong> ある言語から別の言語へ翻訳できます。</li>\n",
       "<li><strong>コード生成:</strong> プログラミングコードを生成できます。</li>\n",
       "<li><strong>文章の校正・添削:</strong> 文法的な誤りや不自然な表現を修正できます。</li>\n",
       "<li><strong>感情分析:</strong> テキストに含まれる感情を分析できます。</li>\n",
       "<li><strong>チャットボット:</strong> 人間と自然な会話をすることができます。</li>\n",
       "</ul>\n",
       "<p><strong>LLMの仕組み（ざっくりとした説明）：</strong></p>\n",
       "<ol>\n",
       "<li><strong>学習:</strong> 大量のテキストデータを読み込ませ、単語の並びや文法、意味などを学習させます。</li>\n",
       "<li><strong>予測:</strong> 学習した情報に基づいて、次に続く単語や文章を予測します。</li>\n",
       "<li><strong>生成:</strong> 予測を繰り返すことで、自然な文章を生成します。</li>\n",
       "</ol>\n",
       "<p><strong>代表的なLLM:</strong></p>\n",
       "<ul>\n",
       "<li><strong>GPTシリーズ (OpenAI):</strong> ChatGPT, GPT-3, GPT-4 など</li>\n",
       "<li><strong>LaMDA (Google):</strong> Bard (現在は Gemini)</li>\n",
       "<li><strong>BERT (Google):</strong>  検索エンジンの性能向上に貢献</li>\n",
       "<li><strong>Llamaシリーズ (Meta):</strong> Llama 2, Llama 3 など</li>\n",
       "</ul>\n",
       "<p><strong>LLMのメリット:</strong></p>\n",
       "<ul>\n",
       "<li><strong>高度な文章生成能力:</strong> 人間が書いたような自然で流暢な文章を生成できます。</li>\n",
       "<li><strong>汎用性の高さ:</strong> 様々な言語タスクに対応できます。</li>\n",
       "<li><strong>創造性の高さ:</strong> 新しいアイデアや表現を生み出すことができます。</li>\n",
       "<li><strong>自動化:</strong> テキスト作成や翻訳などの作業を自動化できます。</li>\n",
       "</ul>\n",
       "<p><strong>LLMの課題:</strong></p>\n",
       "<ul>\n",
       "<li><strong>大量の計算資源:</strong> 学習や実行に莫大な計算資源が必要です。</li>\n",
       "<li><strong>偏り:</strong> 学習データに偏りがあると、生成される文章にも偏りが生じる可能性があります。</li>\n",
       "<li><strong>誤情報の拡散:</strong> 事実に基づかない情報や有害な情報を生成する可能性があります。</li>\n",
       "<li><strong>倫理的な問題:</strong> 著作権侵害、プライバシー侵害などの倫理的な問題が懸念されます。</li>\n",
       "<li><strong>ハルシネーション:</strong> 事実に基づかない情報を生成してしまう（もっともらしい嘘をつく）ことがあります。</li>\n",
       "</ul>\n",
       "<p><strong>LLMの利用例:</strong></p>\n",
       "<ul>\n",
       "<li><strong>カスタマーサポート:</strong> チャットボットによる問い合わせ対応</li>\n",
       "<li><strong>コンテンツ作成:</strong> ブログ記事、広告文、メールマガジンなどの作成</li>\n",
       "<li><strong>翻訳:</strong> グローバル展開する企業のWebサイトやドキュメントの翻訳</li>\n",
       "<li><strong>教育:</strong> 個別学習支援、論文作成支援</li>\n",
       "<li><strong>研究開発:</strong> 新薬開発、素材開発</li>\n",
       "</ul>\n",
       "<p><strong>まとめ:</strong></p>\n",
       "<p>LLMは、高度な文章生成能力と汎用性を持つ画期的なAIモデルですが、課題も多く存在します。今後の技術開発によって、これらの課題が克服され、より安全で有用なLLMが普及することが期待されます。</p>\n",
       "<p><strong>さらに詳しく知りたい場合:</strong></p>\n",
       "<ul>\n",
       "<li><strong>LLMの種類と特徴:</strong> 各LLM（GPT, LaMDA, BERTなど）のアーキテクチャ、学習データ、得意分野などを調べてみてください。</li>\n",
       "<li><strong>LLMの応用事例:</strong> 様々な業界でのLLMの活用事例を調べてみてください。</li>\n",
       "<li><strong>LLMの倫理問題:</strong> LLMの偏り、誤情報の拡散、著作権侵害などの倫理問題について考えてみてください。</li>\n",
       "</ul>\n",
       "<p>この説明がLLMについて理解する上で役立つことを願っています。何か質問があれば、遠慮なく聞いてください。</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#------------------------------------------\n",
    "#　通常の生成: 全ての文章の生成が完了した後に、生成した文章をまとめて出力する。\n",
    "#------------------------------------------\n",
    "result = gemini.generate_content(\"LLMについて説明してください。\")\n",
    "display_markdown(result.text) # ← display_markdown(result.content)で.content ではなく .text を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c14e247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LLM（大規模言語モデル）とは？\n",
      "\n",
      "LLM（Large Language Model、大規模言語モデル）とは、**大量のテキストデータを学習し、人間が使う言語を理解し、生成することができるAIモデル**です。その名の通り、非常に大規模な学習データとパラメータ数を持つことが特徴です。\n",
      "\n",
      "**簡単に言うと、「言葉を理解し、操ることができる賢いコンピュータプログラム」です。**\n",
      "\n",
      "**LLMの主な特徴:**\n",
      "\n",
      "*   **大規模なデータセット:** インターネット上のテキストデータ、書籍、論文など、膨大な量のテキストデータを学習しています。\n",
      "*   **高度な言語理解能力:** テキストの意味を理解し、文法的な構造や文脈を把握することができます。\n",
      "*   **テキスト生成能力:** 人間が書くような自然な文章を生成することができます。\n",
      "*   **多様なタスクに対応:** 質問応答、翻訳、要約、文章作成、コード生成など、様々なタスクを実行できます。\n",
      "*   **文脈理解:** 長い文章や会話の流れを理解し、文脈に沿った応答を生成できます。\n",
      "*   **知識獲得:** 学習データに含まれる情報を知識として蓄積し、質問に答えたり、推論を行ったりすることができます。\n",
      "\n",
      "**LLMの仕組み（概要）:**\n",
      "\n",
      "LLMは、主に**Transformer**と呼ばれるニューラルネットワークアーキテクチャに基づいて構築されています。Transformerは、テキスト内の単語間の関係性を効率的に捉えることができるため、言語理解と生成に非常に適しています。\n",
      "\n",
      "学習プロセスでは、LLMは大量のテキストデータを用いて、単語の出現パターンや文法規則などを学習します。具体的には、ある単語の後に続く可能性の高い単語を予測したり、文脈に合った単語を選択したりする能力を習得します。\n",
      "\n",
      "**LLMの活用例:**\n",
      "\n",
      "*   **チャットボット:** ユーザーからの質問に答えたり、会話をしたりすることができます。\n",
      "*   **文章作成支援:** ブログ記事、メール、レポートなどの文章作成を支援します。\n",
      "*   **翻訳:** ある言語の文章を別の言語に翻訳します。\n",
      "*   **要約:** 長い文章を要約します。\n",
      "*   **プログラミング:** コードを生成したり、バグを修正したりします。\n",
      "*   **検索エンジン:** ユーザーの検索意図を理解し、より関連性の高い検索結果を提供します。\n",
      "*   **教育:** 学習教材の作成、個別指導、課題の採点などを支援します。\n",
      "*   **コンテンツ生成:** ストーリー、詩、音楽などを生成します。\n",
      "*   **医療:** 診断支援、治療計画の作成などを支援します。\n",
      "\n",
      "**LLMの課題:**\n",
      "\n",
      "*   **計算コスト:** 大規模なモデルの学習と実行には、膨大な計算資源が必要です。\n",
      "*   **バイアス:** 学習データに含まれる偏りが、生成されるテキストに反映されることがあります。\n",
      "*   **倫理的な問題:** 誤情報や有害なコンテンツの生成、プライバシー侵害などのリスクがあります。\n",
      "*   **幻覚（ハルシネーション）:** 事実に基づかない情報を生成することがあります。\n",
      "*   **解釈可能性の欠如:** モデルの意思決定プロセスがブラックボックスであるため、なぜ特定の応答を生成したのか理解することが難しい場合があります。\n",
      "\n",
      "**代表的なLLM:**\n",
      "\n",
      "*   **GPTシリーズ (OpenAI):** GPT-3, GPT-4など\n",
      "*   **LaMDA (Google):**\n",
      "*   **BERT (Google):**\n",
      "*   **PaLM (Google):**\n",
      "*   **Llama (Meta):**\n",
      "\n",
      "**まとめ:**\n",
      "\n",
      "LLMは、言語処理の分野に革命をもたらしつつある非常に強力なツールです。様々な分野での応用が期待される一方で、課題も多く存在します。今後の技術発展と倫理的な配慮が重要です。\n",
      "\n",
      "**より詳細な説明が必要な場合は、知りたい具体的な内容を教えてください。**\n",
      "\n",
      "例えば、以下のような内容について詳しく説明できます。\n",
      "\n",
      "*   Transformerアーキテクチャの詳細\n",
      "*   LLMの学習方法\n",
      "*   LLMの評価指標\n",
      "*   LLMのセキュリティ\n",
      "*   特定のLLMモデルの詳細 (GPT-3, Llamaなど)\n",
      "\n",
      "ご希望に応じて、より深く掘り下げて説明します。\n"
     ]
    }
   ],
   "source": [
    "# Google の google.generativeai ライブラリにおける GenerativeModel（例：Gemini 2.0 Flash）は、\n",
    "# 現時点（2025年時点）では stream() メソッドをサポートしていません。\n",
    "# つまり、Gemini API では stream() は存在しないため、直接使おうとするとこのエラーになります。\n",
    "\n",
    "# 以下は擬似的に「文字を1文字ずつ表示」する方法\n",
    "import time\n",
    "\n",
    "response = gemini.generate_content(\"LLMについて説明してください。\")\n",
    "\n",
    "for char in response.text:\n",
    "    print(char, end='', flush=True)\n",
    "    time.sleep(0.01)  # 少しずつ出力（擬似的ストリーミング）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdd30f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>20 - 2 - 8 = 10</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>3 + 25 - 8 = 28 - 8 = 20</p>\n",
       "<p>答えは <strong>20</strong> です。</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>29 + 3 - 2 = 32 - 2 = 30</p>\n",
       "<p>したがって、答えは <strong>30</strong> です。</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display, HTML\n",
    "import markdown\n",
    "\n",
    "# APIキーの設定\n",
    "genai.configure(api_key=\"AIzaSyA7ff88iym-pPHbjSKf-x6ddwFYzULZ9ec\")\n",
    "gemini = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# Markdown表示用（Jupyterなどで使う場合）\n",
    "def display_markdown(md_text):\n",
    "    html_output = markdown.markdown(md_text)\n",
    "    display(HTML(html_output))\n",
    "\n",
    "# 通常のテキスト生成\n",
    "prompts = [\n",
    "    \"20-2-8を計算して\",\n",
    "    \"3+25-8を計算して\",\n",
    "    \"29+3-2を計算して\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = gemini.generate_content(prompt)\n",
    "    display_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c87571c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>はい、LLM（Large Language Model：大規模言語モデル）について、具体例を交えながらわかりやすく説明しますね。</p>\n",
       "<p><strong>LLMとは？</strong></p>\n",
       "<p>LLMとは、簡単に言うと、大量のテキストデータを学習することで、人間が書いた文章のように自然な文章を生成したり、質問に答えたり、様々なタスクを実行できるAIモデルのことです。「大規模」という名前の通り、非常に多くのデータとパラメータを使って学習されているのが特徴です。</p>\n",
       "<p><strong>LLMの仕組みのイメージ</strong></p>\n",
       "<p>LLMは、大量のテキストデータを「単語のつながりのパターン」として学習します。例えば、「猫」という単語の後に「が」が来やすいとか、「犬」と「散歩」という単語が一緒に使われやすいといったパターンを、膨大な量のデータから統計的に学習するのです。</p>\n",
       "<p>まるで、子供がたくさんの本を読んだり、人との会話を聞いたりして、言葉の使い方を覚えていくのに似ています。</p>\n",
       "<p><strong>LLMができること（具体例付き）</strong></p>\n",
       "<ul>\n",
       "<li><strong>文章の生成:</strong><ul>\n",
       "<li><strong>例:</strong> 「猫が昼寝をしている様子を短く描写してください」と指示すると、「日の当たる窓辺で、白猫が丸くなってすやすやと眠っている。時折、小さないびきが聞こえる。」のような文章を生成します。まるで人が書いたかのような自然な文章です。</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>質問応答:</strong><ul>\n",
       "<li><strong>例:</strong> 「日本の首都はどこですか？」と質問すると、「日本の首都は東京です。」と正確に答えます。</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>翻訳:</strong><ul>\n",
       "<li><strong>例:</strong> 「Hello, how are you?」を日本語に翻訳するように指示すると、「こんにちは、お元気ですか？」と翻訳します。</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>要約:</strong><ul>\n",
       "<li><strong>例:</strong> 長いニュース記事をLLMに入力すると、記事の要点を簡潔にまとめてくれます。時間がないときに便利ですね。</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>文章の校正・添削:</strong><ul>\n",
       "<li><strong>例:</strong> タイプミスが多い文章をLLMに入力すると、文法やスペルミスを修正してくれます。</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>プログラミングコードの生成:</strong><ul>\n",
       "<li><strong>例:</strong> 「Pythonでフィボナッチ数列を計算するコードを書いて」と指示すると、正しいコードを生成してくれます。（ただし、複雑なコードの場合は、生成されたコードを検証する必要があります。）</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>チャットボット:</strong><ul>\n",
       "<li><strong>例:</strong> LLMを搭載したチャットボットは、人間と自然な会話をすることができます。カスタマーサポートや相談相手として活用されています。</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<p><strong>LLMの課題</strong></p>\n",
       "<ul>\n",
       "<li><strong>嘘をつく（ハルシネーション）:</strong> LLMは、学習データに基づいて文章を生成しますが、必ずしも真実を述べているとは限りません。事実と異なることをもっともらしく答えることがあります。（例：存在しない歴史上の人物について語ったり、間違った情報をあたかも事実のように述べたりします。）</li>\n",
       "<li><strong>偏見を含む:</strong> 学習データに偏りがあると、生成される文章にも偏見が含まれることがあります。（例：特定の職業を特定の性別と関連付けるなど。）</li>\n",
       "<li><strong>著作権の問題:</strong> LLMが生成した文章が、既存の著作物を侵害する可能性があります。</li>\n",
       "<li><strong>倫理的な問題:</strong> LLMを悪用して、偽情報を拡散したり、詐欺を働いたりする可能性があります。</li>\n",
       "</ul>\n",
       "<p><strong>LLMの進化とこれから</strong></p>\n",
       "<p>LLMは日々進化しており、より自然で正確な文章を生成できるようになっています。今後は、さらに多くのタスクを実行できるようになり、私たちの生活や仕事に大きな影響を与えると考えられます。しかし、上記のような課題を克服し、倫理的に安全に利用するための取り組みも重要です。</p>\n",
       "<p><strong>まとめ</strong></p>\n",
       "<p>LLMは、大量のテキストデータを学習した、非常に強力なAIモデルです。様々なタスクを実行できますが、課題も存在します。LLMを理解し、適切に活用することで、私たちの生活や仕事をより豊かにすることができるでしょう。</p>\n",
       "<p>ご不明な点があれば、お気軽にご質問ください。</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Gemini（google.generativeai.GenerativeModel）の generate_content() は、リスト形式のプロンプトは受け取るが、要素は単なる文字列である必要がある。\n",
    "prompt = \"\"\"\n",
    "あなたは日本語で具体例を使いながら、わかりやすく回答するチャットボットです。\n",
    "以下の質問に答えてください。\n",
    "\n",
    "質問: LLMについて教えてください。\n",
    "\"\"\"\n",
    "response = gemini.generate_content(prompt)\n",
    "display_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c956d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "#　チャット履歴とLLMを連携したchain_with_historyを作成\n",
    "#------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de547244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Gemini の LangChain 対応モデル\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# プロンプト定義\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは日本語で会話するチャットボットです\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# チェーンの構成\n",
    "chain = prompt | gemini | StrOutputParser()\n",
    "\n",
    "# メモリの定義\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "# チェーンと履歴を組み合わせた実行可能なオブジェクト\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: history,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d92141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "# chain_with_historyを呼び出し、チャット履歴を参照したテキスト生成を行う。終了するには「終了」と入力します。\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84879c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャットを終了します。\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# ✅ APIキーの設定（安全のため環境変数で管理すべき）\n",
    "genai.configure(api_key=\"AIzaSyA7ff88iym-pPHbjSKf-x6ddwFYzULZ9ec\")\n",
    "\n",
    "# ✅ モデルの初期化\n",
    "gemini = genai.GenerativeModel(\"gemini-2.0-flash\")  # または \"gemini-pro\"\n",
    "\n",
    "# ✅ チャットセッションを開始（履歴あり）\n",
    "chat = gemini.start_chat(history=[])\n",
    "\n",
    "# ✅ チャットループ\n",
    "while True:\n",
    "    user_input = input(\"プロンプトを入力してください（終了するには '終了' と入力）: \")\n",
    "\n",
    "    if user_input.strip().lower() in [\"終了\", \"exit\", \"quit\"]:\n",
    "        print(\"チャットを終了します。\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # ✅ 履歴付きチャットセッションにメッセージ送信\n",
    "        response = chat.send_message(user_input)\n",
    "\n",
    "        # ✅ 応答表示\n",
    "        print(\"\\nLLMの応答:\")\n",
    "        print(response.text)\n",
    "        print(\"\\n\")  # 空行で見やすくする\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ エラーが発生しました:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 質問: MMLUベンチマークにおけるGPT4の性能を日本語で教えてください。\n",
      "\n",
      "## RAGを使う場合の生成結果:\n",
      "提供された情報には、GPT-4がMMLUベンチマークを多言語に翻訳してテストした結果、GPT-3.5やChinchilla、PaLMといった既存の言語モデルの英語での性能を上回ったことが記載されています。ラトビア語、ウェールズ語、スワヒリ語といった低リソース言語でも同様の結果が得られたことが明記されています。\n",
      "\n",
      "しかし、日本語でのGPT-4の性能については、この情報には具体的に記載されていません。そのため、日本語での性能に関する情報は提供できません。\n"
     ]
    }
   ],
   "source": [
    "#RAG\n",
    "\n",
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"MyLangChainBot/1.0\"\n",
    "\n",
    "# --- LangChain Core コンポーネント ---\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser # LangChain 0.1.0 以降は langchain.schema.output_parser ではなくこちらを推奨\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.documents import Document # 必要に応じてインポート（今回は直接使用していませんが、ドキュメント操作でよく使われます）\n",
    "\n",
    "# --- LangChain Community コンポーネント (外部連携) ---\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.chat_models import ChatGoogleGenerativeAI # コメントアウトまたは削除\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # こちらを使用\n",
    "\n",
    "# --- その他のライブラリ ---\n",
    "from bs4 import SoupStrainer # Web スクレイピングに必要\n",
    "# from IPython.display import display_markdown # Jupyter環境などでMarkdown表示に使う場合\n",
    "\n",
    "# --- ドキュメントの読み込みと前処理 ---\n",
    "# ⚠️ 注意: 大規模なWebサイトをクロールする際は、robots.txt を確認し、\n",
    "# サイトの利用規約に従ってください。過度なリクエストは避けるべきです。\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://arxiv.org/html/2303.08774v6\",),\n",
    "    # SoupStrainer を使うことで、特定のHTML要素のみを解析し、効率化できます。\n",
    "    bs_kwargs={'parse_only': SoupStrainer(class_=\"ltx_section\")}\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# ドキュメントをチャンクに分割することで、LLMのコンテキストウィンドウに収まるようにします。\n",
    "# チャンクサイズとオーバーラップは、ドキュメントの性質やタスクによって調整が必要です。\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # チャンクの最大文字数\n",
    "    chunk_overlap=200 # チャンク間の重複文字数。これにより、文脈の途切れを防ぎます。\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# --- 埋め込みモデルとベクトルストアの準備 ---\n",
    "# Hugging Face の埋め込みモデルを使用。多言語対応モデルは日本語RAGに特に有効です。\n",
    "# ローカルでモデルをダウンロードして実行するため、インターネット接続は初回のみ必要です。\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\")\n",
    "\n",
    "# ChromaDB はローカルで動作する軽量なベクトルストアで、開発や小規模なアプリケーションに適しています。\n",
    "# persist_directory を指定することで、次回以降はデータベースを再構築せずに利用できます。\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "# ベクトルストアがディスクに永続化されたことを明示的に伝えるために `persist()` を呼び出すことを推奨します。\n",
    "vectorstore.persist()\n",
    "\n",
    "# --- LLM とプロンプトの準備 ---\n",
    "# Google Gemini の LLM を定義。\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    google_api_key=\"AIzaSyA7ff88iym-pPHbjSKf-x6ddwFYzULZ9ec\"  # ここに取得したAPIキーを入力\n",
    ")\n",
    "# from langchain_openai import ChatOpenAI # OpenAIを使う場合の例\n",
    "\n",
    "# LangChain Hub から RAG 用プロンプトを取得。\n",
    "# これらのプロンプトは、RAGタスクに最適化されており、効果的な応答生成を助けます。\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "あなたはユーザーの質問に答えるAIアシスタントです。\n",
    "提供された情報のみに基づいて、質問に答えてください。\n",
    "情報が不足している場合は、その旨を伝えてください。\n",
    "\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "# 💡 LangChain Hub (pull(\"rlm/rag-prompt\")) も便利ですが、\n",
    "# プロンプトのカスタマイズ性を考慮し、ここでは `ChatPromptTemplate.from_template` を使用する例を示します。\n",
    "# 必要に応じて Hub のプロンプトをプルして内容を確認し、上記のように調整することも可能です。\n",
    "\n",
    "# ベクトルストアの retriever を定義。\n",
    "# 検索結果の数を k で制御できます。\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # 関連ドキュメントを3つ取得する例\n",
    "\n",
    "# --- RAG チェーンの構築 ---\n",
    "# ドキュメントをLLMのプロンプトに組み込むために整形する関数\n",
    "def format_docs(docs):\n",
    "    # 各ドキュメントのページ内容を結合し、区切り文字を挿入します。\n",
    "    # これにより、LLMがコンテキストを理解しやすくなります。\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# LangChain Expression Language (LCEL) を用いた RAG チェーンの構成。\n",
    "# これにより、チェーンの可読性とモジュール性が向上します。\n",
    "RAG_chain = (\n",
    "    # \"context\" には retriever で取得したドキュメントを format_docs で整形したものを渡します。\n",
    "    # \"question\" にはユーザーからの質問をそのまま渡します。\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt # 質問とコンテキストをプロンプトテンプレートに渡します。\n",
    "    | llm    # プロンプトをLLMに入力し、応答を生成します。\n",
    "    | StrOutputParser() # LLMの出力を文字列としてパースします。\n",
    ")\n",
    "\n",
    "# --- RAG を使った生成の実行 ---\n",
    "# ユーザーの質問に対して RAG チェーンを実行し、関連情報に基づいた回答を得ます。\n",
    "question = \"MMLUベンチマークにおけるGPT4の性能を日本語で教えてください。\"\n",
    "print(f\"## 質問: {question}\\n\")\n",
    "RAG_result = RAG_chain.invoke(question)\n",
    "print(\"## RAGを使う場合の生成結果:\\n\" + RAG_result)\n",
    "\n",
    "# display_markdown(\"## RAGを使う場合の生成結果:\\n\" + RAG_result) # Jupyter 環境以外では動作しないためコメントアウト\n",
    "\n",
    "# --- 追加の考慮事項 ---\n",
    "# - ストリーミング出力: 大規模な応答の場合、ストリーミング出力を使うとユーザー体験が向上します。\n",
    "#   `RAG_chain.stream(question)` を使用し、チャンクごとに処理できます。\n",
    "# - エラーハンドリング: API呼び出しやファイル操作におけるエラーハンドリングを追加することを検討してください。\n",
    "# - キャッシュ: 同じ質問に対する応答をキャッシュすることで、APIコストを削減し、応答速度を向上できます。\n",
    "# - ログ: アプリケーションの動作を追跡するために、ログ記録を設定することを推奨します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba551ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 質問: MMLUベンチマークにおけるGPT4の性能を日本語で教えてください。\n",
      "\n",
      "## RAGを使う場合の生成結果 (ストリーミング):\n",
      "\n",
      "提供された情報には、GPT-4がMMLUベンチマークを多数の言語に翻訳してテストし、GPT-3.5、Chinchilla、PaLMといった既存の言語モデルよりも多くの言語で優れた性能を示したことが記載されています。ラトビア語、ウェールズ語、スワヒリ語のような低リソース言語でも良い結果を出したことが明記されています。\n",
      "\n",
      "しかし、日本語でのGPT-4の性能については具体的に言及されていません。そのため、日本語での性能に関する情報は提供されたテキストからは判断できません。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RAG ストリーミング出力\n",
    "\n",
    "import os\n",
    "# 環境変数は、APIキーなど機密情報を扱う際に特に重要です。\n",
    "# 今回の USER_AGENT は LangChain の挙動に影響しますが、機密情報ではありません。\n",
    "os.environ[\"USER_AGENT\"] = \"MyLangChainBot/1.0\"\n",
    "\n",
    "# --- LangChain Core コンポーネント ---\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser # LangChain 0.1.0 以降は langchain.schema.output_parser ではなくこちらを推奨\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.documents import Document # 必要に応じてインポート（今回は直接使用していませんが、ドキュメント操作でよく使われます）\n",
    "\n",
    "# --- LangChain Community コンポーネント (外部連携) ---\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.chat_models import ChatGoogleGenerativeAI # コメントアウトまたは削除\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # こちらを使用\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # RecursiveCharacterTextSplitterのインポートが抜けていました\n",
    "\n",
    "# --- その他のライブラリ ---\n",
    "from bs4 import SoupStrainer # Web スクレイピングに必要\n",
    "# from IPython.display import display_markdown # Jupyter環境などでMarkdown表示に使う場合\n",
    "\n",
    "# --- ドキュメントの読み込みと前処理 ---\n",
    "# ⚠️ 注意: 大規模なWebサイトをクロールする際は、robots.txt を確認し、\n",
    "# サイトの利用規約に従ってください。過度なリクエストは避けるべきです。\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://arxiv.org/html/2303.08774v6\",),\n",
    "    # SoupStrainer を使うことで、特定のHTML要素のみを解析し、効率化できます。\n",
    "    bs_kwargs={'parse_only': SoupStrainer(class_=\"ltx_section\")}\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# ドキュメントをチャンクに分割することで、LLMのコンテキストウィンドウに収まるようにします。\n",
    "# チャンクサイズとオーバーラップは、ドキュメントの性質やタスクによって調整が必要です。\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # チャンクの最大文字数\n",
    "    chunk_overlap=200 # チャンク間の重複文字数。これにより、文脈の途切れを防ぎます。\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# --- 埋め込みモデルとベクトルストアの準備 ---\n",
    "# Hugging Face の埋め込みモデルを使用。多言語対応モデルは日本語RAGに特に有効です。\n",
    "# ローカルでモデルをダウンロードして実行するため、インターネット接続は初回のみ必要です。\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\")\n",
    "\n",
    "# ChromaDB はローカルで動作する軽量なベクトルストアで、開発や小規模なアプリケーションに適しています。\n",
    "# persist_directory を指定することで、次回以降はデータベースを再構築せずに利用できます。\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "# ベクトルストアがディスクに永続化されたことを明示的に伝えるために `persist()` を呼び出すことを推奨します。\n",
    "vectorstore.persist()\n",
    "\n",
    "# --- LLM とプロンプトの準備 ---\n",
    "# Google Gemini の LLM を定義。\n",
    "# ⚠️ 注意: APIキーをコードに直接記述することは、セキュリティ上非推奨です。\n",
    "# 環境変数を使用するか、.env ファイルから読み込むことを強く推奨します。\n",
    "# 例: os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\" を事前に設定。\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    google_api_key=\"AIzaSyA7ff88iym-pPHbjSKf-x6ddwFYzULZ9ec\", # ここに取得したAPIキーを入力\n",
    "    temperature=0.7 # 温度設定を追加。回答の多様性を制御します。\n",
    ")\n",
    "# from langchain_openai import ChatOpenAI # OpenAIを使う場合の例\n",
    "\n",
    "# LangChain Hub から RAG 用プロンプトを取得。\n",
    "# これらのプロンプトは、RAGタスクに最適化されており、効果的な応答生成を助けます。\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "あなたはユーザーの質問に答えるAIアシスタントです。\n",
    "提供された情報のみに基づいて、質問に答えてください。\n",
    "情報が不足している場合は、その旨を伝えてください。\n",
    "\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "# 💡 LangChain Hub (pull(\"rlm/rag-prompt\")) も便利ですが、\n",
    "# プロンプトのカスタマイズ性を考慮し、ここでは `ChatPromptTemplate.from_template` を使用する例を示します。\n",
    "# 必要に応じて Hub のプロンプトをプルして内容を確認し、上記のように調整することも可能です。\n",
    "\n",
    "# ベクトルストアの retriever を定義。\n",
    "# 検索結果の数を k で制御できます。\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # 関連ドキュメントを3つ取得する例\n",
    "\n",
    "# --- RAG チェーンの構築 ---\n",
    "# ドキュメントをLLMのプロンプトに組み込むために整形する関数\n",
    "def format_docs(docs):\n",
    "    # 各ドキュメントのページ内容を結合し、区切り文字を挿入します。\n",
    "    # これにより、LLMがコンテキストを理解しやすくなります。\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# LangChain Expression Language (LCEL) を用いた RAG チェーンの構成。\n",
    "# これにより、チェーンの可読性とモジュール性が向上します。\n",
    "RAG_chain = (\n",
    "    # \"context\" には retriever で取得したドキュメントを format_docs で整形したものを渡します。\n",
    "    # \"question\" にはユーザーからの質問をそのまま渡します。\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt # 質問とコンテキストをプロンプトテンプレートに渡します。\n",
    "    | llm    # プロンプトをLLMに入力し、応答を生成します。\n",
    "    | StrOutputParser() # LLMの出力を文字列としてパースします。\n",
    ")\n",
    "\n",
    "# --- RAG を使った生成の実行 (ストリーミング版) ---\n",
    "# ユーザーの質問に対して RAG チェーンを実行し、関連情報に基づいた回答を得ます。\n",
    "question = \"MMLUベンチマークにおけるGPT4の性能を日本語で教えてください。\"\n",
    "print(f\"## 質問: {question}\\n\")\n",
    "print(\"## RAGを使う場合の生成結果 (ストリーミング):\\n\")\n",
    "\n",
    "# ✅ invoke() の代わりに stream() を使用し、逐次結果を出力\n",
    "for chunk in RAG_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True) # chunk を受け取るたびに表示し、バッファをフラッシュ\n",
    "print(\"\\n\") # 最後に改行を追加して整形\n",
    "\n",
    "# --- 追加の考慮事項 ---\n",
    "# - ストリーミング出力: 大規模な応答の場合、ストリーミング出力を使うとユーザー体験が向上します。\n",
    "#   `RAG_chain.stream(question)` を使用し、チャンクごとに処理できます。\n",
    "# - エラーハンドリング: API呼び出しやファイル操作におけるエラーハンドリングを追加することを検討してください。\n",
    "# - キャッシュ: 同じ質問に対する応答をキャッシュすることで、APIコストを削減し、応答速度を向上できます。\n",
    "# - ログ: アプリケーションの動作を追跡するために、ログ記録を設定することを推奨します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84426d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 質問: 今日のニュースを教えてください。\n",
      "\n",
      "## RAGを使う場合の生成結果 (ストリーミング):\n",
      "\n",
      "提供された情報に基づくと、今日のニュースは以下のとおりです。\n",
      "\n",
      "* **各地で30℃超え 熱中症のリスク大**\n",
      "* **東電株主訴訟 揺れる司法判断**\n",
      "* **コメ適正価格は3000円 生産者団体**\n",
      "* **トンネルで車が正面衝突 男性死亡**\n",
      "* **電話が苦手な20代 企業の対応は**\n",
      "* **ガルベス氏 長嶋さんに謝りたい**\n",
      "* **6月の風物詩 大河クランクイン**\n",
      "* **西内まりや SNS更新「お元気で」**\n",
      "\n",
      "その他、小泉農水相の発言による米の備蓄放出に関する話題や、元フジ女子アナへの誹謗中傷、元なでしこ宮間あやさんの引退後の活動、長嶋茂雄さん追悼特番への不満、タイで日本人男性の死刑求刑など、様々なニュースが報じられています。\n",
      "\n",
      "アクセスランキング上位としては、ガルベス氏が長嶋茂雄さんについて語った記事や、無期懲役囚の終身刑化に関する報道、宮沢りえさんの話題などが注目を集めているようです。\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RAG ページの全てのコンテンツを解析し、それをドキュメントとしてロード\n",
    "\n",
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"MyLangChainBot/1.0\"\n",
    "\n",
    "# --- LangChain Core コンポーネント ---\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# --- LangChain Community コンポーネント (外部連携) ---\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # RecursiveCharacterTextSplitterのインポート\n",
    "\n",
    "# --- その他のライブラリ ---\n",
    "from bs4 import SoupStrainer # Web スクレイピングに必要ですが、今回は使用しないので残しておくか削除するかはお好みで。\n",
    "\n",
    "# --- ドキュメントの読み込みと前処理 ---\n",
    "# ⚠️ 注意: 大規模なWebサイトをクロールする際は、robots.txt を確認し、\n",
    "# サイトの利用規約に従ってください。過度なリクエストは避けるべきです。\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://news.yahoo.co.jp/\",),\n",
    "    # 💡 サイト全体から情報を取得するため、bs_kwargs パラメータを削除しました。\n",
    "    # これにより、WebBaseLoaderはWebページのHTML全体を解析します。\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# ドキュメントをチャンクに分割することで、LLMのコンテキストウィンドウに収まるようにします。\n",
    "# チャンクサイズとオーバーラップは、ドキュメントの性質やタスクによって調整が必要です。\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # チャンクの最大文字数\n",
    "    chunk_overlap=200 # チャンク間の重複文字数。これにより、文脈の途切れを防ぎます。\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# --- 埋め込みモデルとベクトルストアの準備 ---\n",
    "# Hugging Face の埋め込みモデルを使用。多言語対応モデルは日本語RAGに特に有効です。\n",
    "# ローカルでモデルをダウンロードして実行するため、インターネット接続は初回のみ必要です。\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\")\n",
    "\n",
    "# ChromaDB はローカルで動作する軽量なベクトルストアで、開発や小規模なアプリケーションに適しています。\n",
    "# persist_directory を指定することで、次回以降はデータベースを再構築せずに利用できます。\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "# ベクトルストアがディスクに永続化されたことを明示的に伝えるために `persist()` を呼び出すことを推奨します。\n",
    "vectorstore.persist()\n",
    "\n",
    "# --- LLM とプロンプトの準備 ---\n",
    "# Google Gemini の LLM を定義。\n",
    "# ⚠️ 注意: APIキーをコードに直接記述することは、セキュリティ上非推奨です。\n",
    "# 環境変数を使用するか、.env ファイルから読み込むことを強く推奨します。\n",
    "# 例: os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\" を事前に設定。\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    google_api_key=\"AIzaSyA7ff88iym-pPHbjSKf-x6ddwFYzULZ9ec\", # ここに取得したAPIキーを入力\n",
    "    temperature=0.7 # 温度設定を追加。回答の多様性を制御します。\n",
    ")\n",
    "\n",
    "# LangChain Hub から RAG 用プロンプトを取得。\n",
    "# これらのプロンプトは、RAGタスクに最適化されており、効果的な応答生成を助けます。\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "あなたはユーザーの質問に答えるAIアシスタントです。\n",
    "提供された情報のみに基づいて、質問に答えてください。\n",
    "情報が不足している場合は、その旨を伝えてください。\n",
    "\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "# ベクトルストアの retriever を定義。\n",
    "# 検索結果の数を k で制御できます。\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # 関連ドキュメントを3つ取得する例\n",
    "\n",
    "# --- RAG チェーンの構築 ---\n",
    "# ドキュメントをLLMのプロンプトに組み込むために整形する関数\n",
    "def format_docs(docs):\n",
    "    # 各ドキュメントのページ内容を結合し、区切り文字を挿入します。\n",
    "    # これにより、LLMがコンテキストを理解しやすくなります。\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# LangChain Expression Language (LCEL) を用いた RAG チェーンの構成。\n",
    "# これにより、チェーンの可読性とモジュール性が向上します。\n",
    "RAG_chain = (\n",
    "    # \"context\" には retriever で取得したドキュメントを format_docs で整形したものを渡します。\n",
    "    # \"question\" にはユーザーからの質問をそのまま渡します。\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt # 質問とコンテキストをプロンプトテンプレートに渡します。\n",
    "    | llm    # プロンプトをLLMに入力し、応答を生成します。\n",
    "    | StrOutputParser() # LLMの出力を文字列としてパースします。\n",
    ")\n",
    "\n",
    "# --- RAG を使った生成の実行 (ストリーミング版) ---\n",
    "# ユーザーの質問に対して RAG チェーンを実行し、関連情報に基づいた回答を得ます。\n",
    "question = \"今日のニュースを教えてください。\"\n",
    "print(f\"## 質問: {question}\\n\")\n",
    "print(\"## RAGを使う場合の生成結果 (ストリーミング):\\n\")\n",
    "\n",
    "# invoke() の代わりに stream() を使用し、逐次結果を出力\n",
    "for chunk in RAG_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True) # chunk を受け取るたびに表示し、バッファをフラッシュ\n",
    "print(\"\\n\") # 最後に改行を追加して整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d3239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
